{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Missing Data Impact on well known Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Importing the datasets to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  \n",
       "3              40  United-States   >50K  \n",
       "4              30  United-States  <=50K  "
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"wenruliu/adult-income-dataset\") + \"/adult.csv\"\n",
    "adult_df = pd.read_csv(path)\n",
    "adult_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                0\n",
       "workclass          0\n",
       "fnlwgt             0\n",
       "education          0\n",
       "educational-num    0\n",
       "marital-status     0\n",
       "occupation         0\n",
       "relationship       0\n",
       "race               0\n",
       "gender             0\n",
       "capital-gain       0\n",
       "capital-loss       0\n",
       "hours-per-week     0\n",
       "native-country     0\n",
       "income             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"uciml/breast-cancer-wisconsin-data\") \n",
    "path = path + \"/data.csv\"\n",
    "breast_cancer_df= pd.read_csv(path)\n",
    "breast_cancer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           0\n",
       "diagnosis                    0\n",
       "radius_mean                  0\n",
       "texture_mean                 0\n",
       "perimeter_mean               0\n",
       "area_mean                    0\n",
       "smoothness_mean              0\n",
       "compactness_mean             0\n",
       "concavity_mean               0\n",
       "concave points_mean          0\n",
       "symmetry_mean                0\n",
       "fractal_dimension_mean       0\n",
       "radius_se                    0\n",
       "texture_se                   0\n",
       "perimeter_se                 0\n",
       "area_se                      0\n",
       "smoothness_se                0\n",
       "compactness_se               0\n",
       "concavity_se                 0\n",
       "concave points_se            0\n",
       "symmetry_se                  0\n",
       "fractal_dimension_se         0\n",
       "radius_worst                 0\n",
       "texture_worst                0\n",
       "perimeter_worst              0\n",
       "area_worst                   0\n",
       "smoothness_worst             0\n",
       "compactness_worst            0\n",
       "concavity_worst              0\n",
       "concave points_worst         0\n",
       "symmetry_worst               0\n",
       "fractal_dimension_worst      0\n",
       "Unnamed: 32                569\n",
       "dtype: int64"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_df.columns\n",
    "breast_cancer_df = breast_cancer_df.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predefine a missing rate to be used across the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 0.1, 0.3, and 0.5.\n",
    "MISSING_RATE = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the helpers that will be used in model evaluation along Imputers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly it was implemented a function to introduce the Missingness Completely At Random (MCAR) missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdatagen.univariate.uMCAR import uMCAR\n",
    "from mdatagen.univariate.uMAR import uMAR\n",
    "from mdatagen.univariate.uMNAR import uMNAR\n",
    "import pandas as pd\n",
    "\n",
    "def generate_univariate_missingness(X, y, mechanism, missing_rate, x_miss, x_obs=None, seed=None):\n",
    "    \"\"\"\n",
    "    General method to generate univariate missingness using the mdatagen library.\n",
    "    \n",
    "    Parameters:\n",
    "    - X (pd.DataFrame): The feature matrix (independent variables).\n",
    "    - y (pd.Series or np.ndarray): The target variable.\n",
    "    - mechanism (str): The missingness mechanism ('MCAR', 'MAR', or 'MNAR').\n",
    "    - missing_rate (float): The proportion of values to replace with NaN (0.0 to 1.0).\n",
    "    - x_miss (str): The name of the column to introduce missingness into.\n",
    "    - x_obs (str, optional): The name of the column to condition on for MAR/MNAR mechanisms (if required).\n",
    "    - seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame of features with missing values introduced.\n",
    "    - pd.Series or np.ndarray: The unchanged target variable.\n",
    "    \"\"\"\n",
    "    # Ensure the specified column exists\n",
    "    if x_miss not in X.columns:\n",
    "        raise ValueError(f\"Column '{x_miss}' not found in the feature matrix.\")\n",
    "\n",
    "    # Set the random seed for reproducibility\n",
    "    seed = seed or 42\n",
    "\n",
    "    # Initialize the appropriate generator based on the mechanism\n",
    "    if mechanism == 'MCAR':\n",
    "        generator = uMCAR(X=X, y=y, missing_rate=missing_rate, x_miss=x_miss, seed=seed)\n",
    "    elif mechanism == 'MAR':\n",
    "        if not x_obs:\n",
    "            raise ValueError(\"For MAR, you must specify the observed column 'x_obs'.\")\n",
    "        generator = uMAR(X=X, y=y, missing_rate=missing_rate, x_miss=x_miss, x_obs=x_obs)\n",
    "    elif mechanism == 'MNAR':\n",
    "        generator = uMNAR(X=X, y=y, missing_rate=missing_rate, x_miss=x_miss)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mechanism '{mechanism}'. Choose from 'MCAR', 'MAR', or 'MNAR'.\")\n",
    "\n",
    "    # Generate the missing data\n",
    "    if mechanism == 'MCAR':\n",
    "        X_missing = generator.random()\n",
    "    if mechanism == 'MAR':\n",
    "        X_missing = generator.rank()\n",
    "    if mechanism == 'MNAR':\n",
    "        X_missing = generator.run()\n",
    "    # Display missingness details\n",
    "    global_missing_rate = X_missing.isnull().sum().sum() / X_missing.size\n",
    "    print(f\"Global Missing Rate = {global_missing_rate * 100:.2f}%\")\n",
    "    print(\"Missing values per column:\")\n",
    "    print(X_missing.isnull().sum())\n",
    "\n",
    "    return X_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, it is defined a helper `run_logistic_regression` function train, that evaluates a logistic regression model by imputing missing values in the training set and testing data using a specified imputer, fitting the model on the training data, predicting the labels for the testing data, and returning the predictions along with a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def run_logistic_regression(X_train, X_test, y_train, y_test, imputer):\n",
    "    \"\"\"\n",
    "    Runs a Logistic Regression model with preprocessing and a given imputer.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train, X_test: Feature matrices for training and testing.\n",
    "    - y_train, y_test: Target vectors for training and testing.\n",
    "    - imputer: An imputer instance for handling missing values.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels on the test set.\n",
    "    - clf_report: Classification report as a string.\n",
    "    \"\"\"\n",
    "    # Separate columns by data type\n",
    "    numerical_columns = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_columns = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Numerical pipeline\n",
    "    numerical_pipeline = Pipeline([\n",
    "        ('imputer', imputer),  # Use the passed imputer (e.g., KNNImputer or SimpleImputer)\n",
    "        ('scaler', StandardScaler())  # Standardize numerical features\n",
    "    ])\n",
    "\n",
    "    # Categorical pipeline (using OrdinalEncoder instead of OneHotEncoder)\n",
    "    categorical_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing categorical values\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))  # Ordinal encoding for categorical features)  \n",
    "    ])\n",
    "\n",
    "    # Combine the numerical and categorical pipelines\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numerical_pipeline, numerical_columns),\n",
    "        ('cat', categorical_pipeline, categorical_columns)\n",
    "    ])\n",
    "\n",
    "    # Full pipeline with preprocessing and logistic regression\n",
    "    model_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(max_iter=200))\n",
    "    ])\n",
    "\n",
    "    # Fit the model pipeline on the training data\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate the model with a classification report\n",
    "    clf_report = classification_report(y_test, y_pred, output_dict=False)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(clf_report)\n",
    "\n",
    "    return y_pred, clf_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations calculation heatmap to Adult Dataset (ignore the following cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as stat\n",
    "\n",
    "# Function to check if a column is categorical\n",
    "def is_categorical(df, column):\n",
    "    \"\"\"Check if a column in a DataFrame is categorical.\"\"\"\n",
    "    return pd.api.types.is_object_dtype(df[column]) or pd.api.types.is_categorical_dtype(df[column])\n",
    "\n",
    "# Cramér's V function for categorical-categorical association, with missing value handling\n",
    "def cramers_v(x, y):\n",
    "    # Drop rows where either column has missing data\n",
    "    valid = pd.notnull(x) & pd.notnull(y)\n",
    "    x_clean, y_clean = x[valid], y[valid]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(x_clean, y_clean)\n",
    "    chi2 = stat.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    return np.sqrt(chi2 / (n * (min(confusion_matrix.shape) - 1)))\n",
    "\n",
    "# Spearman's rank correlation function, with missing value handling\n",
    "def spearmans_rank(x, y):\n",
    "    \"\"\"Calculate Spearman's rank correlation for continuous or ordinal variables, handling missing values.\"\"\"\n",
    "    # Drop rows where either column has missing data\n",
    "    valid = pd.notnull(x) & pd.notnull(y)\n",
    "    x_clean, y_clean = x[valid], y[valid]\n",
    "    \n",
    "    return stat.spearmanr(x_clean, y_clean)[0]  # Returns the correlation coefficient\n",
    "\n",
    "# Discretize numeric features\n",
    "def discretize_numeric(series, method=\"equal_width\", bins=4):\n",
    "    if method == \"equal_width\":\n",
    "        # Equal-width binning\n",
    "        return pd.cut(series, bins=bins, labels=False, include_lowest=True)\n",
    "    elif method == \"quantile\":\n",
    "        # Equal-frequency binning (quantiles)\n",
    "        return pd.qcut(series, q=bins, labels=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown discretization method: {method}\")\n",
    "\n",
    "# Cramér's V with discretized numeric features\n",
    "def cramers_v_discretized(x, y, method=\"equal_width\", bins=4):\n",
    "    # Determine which column is numeric and which is categorical\n",
    "    if pd.api.types.is_numeric_dtype(x):\n",
    "        x_discretized = discretize_numeric(x, method=method, bins=bins)\n",
    "        return cramers_v(x_discretized, y)\n",
    "    elif pd.api.types.is_numeric_dtype(y):\n",
    "        try:\n",
    "            y_discretized = discretize_numeric(y, method=method, bins=bins)\n",
    "            return cramers_v(x, y_discretized)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        raise ValueError(\"At least one of the inputs should be numeric for discretization.\")\n",
    "\n",
    "# Define the function to calculate the correlation based on data types\n",
    "def calculate_correlations(df, col1, col2):\n",
    "    \"\"\"Calculate correlation between two variables depending on their types.\"\"\"\n",
    "    if is_categorical(df, col1) and is_categorical(df, col2):\n",
    "        # Use Cramér's V for categorical-categorical variables\n",
    "        return cramers_v(df[col1], df[col2])\n",
    "    elif pd.api.types.is_numeric_dtype(df[col1]) and pd.api.types.is_numeric_dtype(df[col2]):\n",
    "        # Use Spearman's rank correlation for continuous-continuous or ordinal variables\n",
    "        return spearmans_rank(df[col1], df[col2])\n",
    "    elif (pd.api.types.is_numeric_dtype(df[col1]) and is_categorical(df, col2)) or \\\n",
    "         (is_categorical(df, col1) and pd.api.types.is_numeric_dtype(df[col2])):\n",
    "        # Use Cramér's V with discretized numeric variables\n",
    "        return cramers_v_discretized(df[col1], df[col2])\n",
    "    # Return NaN when there was an error on calculating the correlation\n",
    "    return np.nan\n",
    "\n",
    "# Correlation matrix calculation for the entire dataset\n",
    "def correlations(df):\n",
    "    df_columns = df.columns\n",
    "    correlation_matrix = pd.DataFrame(index=df_columns, columns=df_columns)\n",
    "\n",
    "    for i, col1 in enumerate(df_columns):\n",
    "        for col2 in df_columns[i+1:]:  # Use i+1 to avoid duplicate pairs and self-correlation\n",
    "            a_corr = calculate_correlations(df, col1, col2)\n",
    "            correlation_matrix.loc[col1, col2] = a_corr\n",
    "            correlation_matrix.loc[col2, col1] = a_corr \n",
    "\n",
    "    correlation_matrix = correlation_matrix.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Matrix for Adult Dataset')\n",
    "    plt.savefig(\"adult_correlations.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the target data for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_adult = adult_df.drop([\"income\"], axis=1)\n",
    "y_adult = adult_df[\"income\"]\n",
    "\n",
    "X_breast_cancer = breast_cancer_df.drop([\"diagnosis\"], axis=1)\n",
    "y_breast_cancer = breast_cancer_df[\"diagnosis\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missingness via Missign Completely At Random (MCAR) mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Missing Rate = 0.94%\n",
      "Missing values per column:\n",
      "id                           0\n",
      "radius_mean                171\n",
      "texture_mean                 0\n",
      "perimeter_mean               0\n",
      "area_mean                    0\n",
      "smoothness_mean              0\n",
      "compactness_mean             0\n",
      "concavity_mean               0\n",
      "concave points_mean          0\n",
      "symmetry_mean                0\n",
      "fractal_dimension_mean       0\n",
      "radius_se                    0\n",
      "texture_se                   0\n",
      "perimeter_se                 0\n",
      "area_se                      0\n",
      "smoothness_se                0\n",
      "compactness_se               0\n",
      "concavity_se                 0\n",
      "concave points_se            0\n",
      "symmetry_se                  0\n",
      "fractal_dimension_se         0\n",
      "radius_worst                 0\n",
      "texture_worst                0\n",
      "perimeter_worst              0\n",
      "area_worst                   0\n",
      "smoothness_worst             0\n",
      "compactness_worst            0\n",
      "concavity_worst              0\n",
      "concave points_worst         0\n",
      "symmetry_worst               0\n",
      "fractal_dimension_worst      0\n",
      "target                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_missing = generate_univariate_missingness(X_breast_cancer, y_breast_cancer.values, mechanism='MCAR', missing_rate=30, x_miss='radius_mean', seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the dataset `X` and labels `y` into training and test sets (`X_train`, `X_test`, `y_train`, `y_test`) while preserving the original class distribution using stratified sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_missing, y_breast_cancer, stratify=y_breast_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      1.00      1.00        90\n",
      "           M       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           1.00       143\n",
      "   macro avg       1.00      1.00      1.00       143\n",
      "weighted avg       1.00      1.00      1.00       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, SimpleImputer(missing_values=np.nan, strategy=\"mean\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploring kNN Imputation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than the mean, we can decide on a `KNNImputer` with `k=5` to fill missing values. Similarly, we train a logistic regression model using the imputed data, and print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      1.00      1.00        90\n",
      "           M       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           1.00       143\n",
      "   macro avg       1.00      1.00      1.00       143\n",
      "weighted avg       1.00      1.00      1.00       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, KNNImputer(n_neighbors=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      1.00      1.00        90\n",
      "           M       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           1.00       143\n",
      "   macro avg       1.00      1.00      1.00       143\n",
      "weighted avg       1.00      1.00      1.00       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, IterativeImputer(max_iter=100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Missing Rate = 0.01%\n",
      "Missing values per column:\n",
      "id                         0\n",
      "radius_mean                0\n",
      "texture_mean               0\n",
      "perimeter_mean             0\n",
      "area_mean                  0\n",
      "smoothness_mean            0\n",
      "compactness_mean           0\n",
      "concavity_mean             0\n",
      "concave points_mean        0\n",
      "symmetry_mean              0\n",
      "fractal_dimension_mean     0\n",
      "radius_se                  0\n",
      "texture_se                 0\n",
      "perimeter_se               0\n",
      "area_se                    0\n",
      "smoothness_se              0\n",
      "compactness_se             0\n",
      "concavity_se               0\n",
      "concave points_se          0\n",
      "symmetry_se                0\n",
      "fractal_dimension_se       0\n",
      "radius_worst               0\n",
      "texture_worst              0\n",
      "perimeter_worst            0\n",
      "area_worst                 0\n",
      "smoothness_worst           0\n",
      "compactness_worst          2\n",
      "concavity_worst            0\n",
      "concave points_worst       0\n",
      "symmetry_worst             0\n",
      "fractal_dimension_worst    0\n",
      "target                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_missing = generate_univariate_missingness(X_breast_cancer, y_breast_cancer.values , mechanism='MCAR',missing_rate=MISSING_RATE,x_miss='compactness_worst', seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_missing, y_breast_cancer.values, stratify=y_breast_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      1.00      1.00        90\n",
      "           M       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           1.00       143\n",
      "   macro avg       1.00      1.00      1.00       143\n",
      "weighted avg       1.00      1.00      1.00       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, SimpleImputer(missing_values=np.nan, strategy=\"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      1.00      1.00        90\n",
      "           M       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           1.00       143\n",
      "   macro avg       1.00      1.00      1.00       143\n",
      "weighted avg       1.00      1.00      1.00       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, KNNImputer(n_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      1.00      1.00        90\n",
      "           M       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           1.00       143\n",
      "   macro avg       1.00      1.00      1.00       143\n",
      "weighted avg       1.00      1.00      1.00       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, IterativeImputer(max_iter=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Univariate Missingness via Missign At Random (MAR) mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_breast_cancer_ = X_breast_cancer.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncorrelation_matrix = X_breast_cancer.corr()\\n\\n# Plot heatmap\\nplt.figure(figsize=(12, 8))\\nsns.heatmap(correlation_matrix, annot=True, cmap=\\'coolwarm\\')\\nplt.title(\"Feature Correlation Heatmap for Breast Cancer Dataset\")\\n\\n# Save the heatmap as an image file\\nplt.savefig(\"breast_cancer_correlation_heatmap.png\", dpi=300, bbox_inches=\\'tight\\')\\n\\n# Display the heatmap\\nplt.show()\\n'"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "correlation_matrix = X_breast_cancer.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Feature Correlation Heatmap for Breast Cancer Dataset\")\n",
    "\n",
    "# Save the heatmap as an image file\n",
    "plt.savefig(\"breast_cancer_correlation_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Missing Rate = 0.01%\n",
      "Missing values per column:\n",
      "id                         0\n",
      "radius_mean                0\n",
      "texture_mean               0\n",
      "perimeter_mean             0\n",
      "area_mean                  0\n",
      "smoothness_mean            0\n",
      "compactness_mean           0\n",
      "concavity_mean             0\n",
      "concave points_mean        0\n",
      "symmetry_mean              0\n",
      "fractal_dimension_mean     0\n",
      "radius_se                  0\n",
      "texture_se                 0\n",
      "perimeter_se               0\n",
      "area_se                    0\n",
      "smoothness_se              0\n",
      "compactness_se             0\n",
      "concavity_se               0\n",
      "concave points_se          0\n",
      "symmetry_se                0\n",
      "fractal_dimension_se       0\n",
      "radius_worst               0\n",
      "texture_worst              0\n",
      "perimeter_worst            0\n",
      "area_worst                 0\n",
      "smoothness_worst           0\n",
      "compactness_worst          2\n",
      "concavity_worst            0\n",
      "concave points_worst       0\n",
      "symmetry_worst             0\n",
      "fractal_dimension_worst    0\n",
      "target                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_missing = generate_univariate_missingness(X=X_breast_cancer_,y=y_breast_cancer.values,mechanism='MAR',missing_rate=MISSING_RATE, x_miss='compactness_worst', x_obs='concavity_worst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_missing, y_breast_cancer, stratify=y_breast_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      1.00      1.00        90\n",
      "           M       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           1.00       143\n",
      "   macro avg       1.00      1.00      1.00       143\n",
      "weighted avg       1.00      1.00      1.00       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, SimpleImputer(missing_values=np.nan, strategy=\"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      1.00      1.00        90\n",
      "           M       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           1.00       143\n",
      "   macro avg       1.00      1.00      1.00       143\n",
      "weighted avg       1.00      1.00      1.00       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, KNNImputer(n_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      1.00      1.00        90\n",
      "           M       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           1.00       143\n",
      "   macro avg       1.00      1.00      1.00       143\n",
      "weighted avg       1.00      1.00      1.00       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, IterativeImputer(max_iter=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adult_ = X_adult.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlations(adult_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Missing Rate = 0.02%\n",
      "Missing values per column:\n",
      "age                  0\n",
      "workclass            0\n",
      "fnlwgt               0\n",
      "education            0\n",
      "educational-num      0\n",
      "marital-status       0\n",
      "occupation           0\n",
      "relationship       147\n",
      "race                 0\n",
      "gender               0\n",
      "capital-gain         0\n",
      "capital-loss         0\n",
      "hours-per-week       0\n",
      "native-country       0\n",
      "target               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_missing = generate_univariate_missingness(X=X_adult_,y=y_adult.values,mechanism='MAR',missing_rate=MISSING_RATE, x_miss='relationship', x_obs='gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_missing, y_adult, stratify=y_adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       1.00      1.00      1.00      9289\n",
      "        >50K       1.00      1.00      1.00      2922\n",
      "\n",
      "    accuracy                           1.00     12211\n",
      "   macro avg       1.00      1.00      1.00     12211\n",
      "weighted avg       1.00      1.00      1.00     12211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, SimpleImputer(missing_values=np.nan, strategy=\"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       1.00      1.00      1.00      9289\n",
      "        >50K       1.00      1.00      1.00      2922\n",
      "\n",
      "    accuracy                           1.00     12211\n",
      "   macro avg       1.00      1.00      1.00     12211\n",
      "weighted avg       1.00      1.00      1.00     12211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, KNNImputer(n_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       1.00      1.00      1.00      9289\n",
      "        >50K       1.00      1.00      1.00      2922\n",
      "\n",
      "    accuracy                           1.00     12211\n",
      "   macro avg       1.00      1.00      1.00     12211\n",
      "weighted avg       1.00      1.00      1.00     12211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, IterativeImputer(max_iter=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Univariate Missingness via Missign Not At Random (MNAR) mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Missing Rate = 0.94%\n",
      "Missing values per column:\n",
      "id                           0\n",
      "radius_mean                  0\n",
      "texture_mean                 0\n",
      "perimeter_mean               0\n",
      "area_mean                    0\n",
      "smoothness_mean              0\n",
      "compactness_mean             0\n",
      "concavity_mean               0\n",
      "concave points_mean          0\n",
      "symmetry_mean                0\n",
      "fractal_dimension_mean       0\n",
      "radius_se                    0\n",
      "texture_se                   0\n",
      "perimeter_se                 0\n",
      "area_se                      0\n",
      "smoothness_se                0\n",
      "compactness_se               0\n",
      "concavity_se                 0\n",
      "concave points_se            0\n",
      "symmetry_se                  0\n",
      "fractal_dimension_se         0\n",
      "radius_worst                 0\n",
      "texture_worst                0\n",
      "perimeter_worst              0\n",
      "area_worst                   0\n",
      "smoothness_worst             0\n",
      "compactness_worst            0\n",
      "concavity_worst            171\n",
      "concave points_worst         0\n",
      "symmetry_worst               0\n",
      "fractal_dimension_worst      0\n",
      "target                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = X_breast_cancer.copy()\n",
    "y = y_breast_cancer\n",
    "\n",
    "X_missing = generate_univariate_missingness(X=X, y=y.values, missing_rate=MISSING_RATE, mechanism='MNAR', x_miss='concavity_worst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_missing, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      1.00      1.00        90\n",
      "           M       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           1.00       143\n",
      "   macro avg       1.00      1.00      1.00       143\n",
      "weighted avg       1.00      1.00      1.00       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, SimpleImputer(missing_values=np.nan, strategy=\"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      1.00      1.00        90\n",
      "           M       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           1.00       143\n",
      "   macro avg       1.00      1.00      1.00       143\n",
      "weighted avg       1.00      1.00      1.00       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, KNNImputer(n_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       1.00      1.00      1.00        90\n",
      "           M       1.00      1.00      1.00        53\n",
      "\n",
      "    accuracy                           1.00       143\n",
      "   macro avg       1.00      1.00      1.00       143\n",
      "weighted avg       1.00      1.00      1.00       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, IterativeImputer(max_iter=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_adult.copy()\n",
    "y = y_adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Missing Rate = 2.00%\n",
      "Missing values per column:\n",
      "age                    0\n",
      "workclass              0\n",
      "fnlwgt                 0\n",
      "education              0\n",
      "educational-num        0\n",
      "marital-status         0\n",
      "occupation             0\n",
      "relationship       14653\n",
      "race                   0\n",
      "gender                 0\n",
      "capital-gain           0\n",
      "capital-loss           0\n",
      "hours-per-week         0\n",
      "native-country         0\n",
      "target                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_missing = generate_univariate_missingness(X=X, y=y.values, mechanism='MNAR',missing_rate=MISSING_RATE,  x_miss='relationship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_missing, y, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       1.00      1.00      1.00      9289\n",
      "        >50K       1.00      1.00      1.00      2922\n",
      "\n",
      "    accuracy                           1.00     12211\n",
      "   macro avg       1.00      1.00      1.00     12211\n",
      "weighted avg       1.00      1.00      1.00     12211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, SimpleImputer(missing_values=np.nan, strategy=\"mean\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       1.00      1.00      1.00      9289\n",
      "        >50K       1.00      1.00      1.00      2922\n",
      "\n",
      "    accuracy                           1.00     12211\n",
      "   macro avg       1.00      1.00      1.00     12211\n",
      "weighted avg       1.00      1.00      1.00     12211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, KNNImputer(n_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       1.00      1.00      1.00      9289\n",
      "        >50K       1.00      1.00      1.00      2922\n",
      "\n",
      "    accuracy                           1.00     12211\n",
      "   macro avg       1.00      1.00      1.00     12211\n",
      "weighted avg       1.00      1.00      1.00     12211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, IterativeImputer(max_iter=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
