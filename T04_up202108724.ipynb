{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing Missing Data Impact on well known Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Importing the datasets to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18          ?  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  \n",
       "3              40  United-States   >50K  \n",
       "4              30  United-States  <=50K  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"wenruliu/adult-income-dataset\") + \"/adult.csv\"\n",
    "adult_df = pd.read_csv(path)\n",
    "adult_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                0\n",
       "workclass          0\n",
       "fnlwgt             0\n",
       "education          0\n",
       "educational-num    0\n",
       "marital-status     0\n",
       "occupation         0\n",
       "relationship       0\n",
       "race               0\n",
       "gender             0\n",
       "capital-gain       0\n",
       "capital-loss       0\n",
       "hours-per-week     0\n",
       "native-country     0\n",
       "income             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"uciml/breast-cancer-wisconsin-data\") \n",
    "path = path + \"/data.csv\"\n",
    "breast_cancer_df= pd.read_csv(path)\n",
    "breast_cancer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           0\n",
       "diagnosis                    0\n",
       "radius_mean                  0\n",
       "texture_mean                 0\n",
       "perimeter_mean               0\n",
       "area_mean                    0\n",
       "smoothness_mean              0\n",
       "compactness_mean             0\n",
       "concavity_mean               0\n",
       "concave points_mean          0\n",
       "symmetry_mean                0\n",
       "fractal_dimension_mean       0\n",
       "radius_se                    0\n",
       "texture_se                   0\n",
       "perimeter_se                 0\n",
       "area_se                      0\n",
       "smoothness_se                0\n",
       "compactness_se               0\n",
       "concavity_se                 0\n",
       "concave points_se            0\n",
       "symmetry_se                  0\n",
       "fractal_dimension_se         0\n",
       "radius_worst                 0\n",
       "texture_worst                0\n",
       "perimeter_worst              0\n",
       "area_worst                   0\n",
       "smoothness_worst             0\n",
       "compactness_worst            0\n",
       "concavity_worst              0\n",
       "concave points_worst         0\n",
       "symmetry_worst               0\n",
       "fractal_dimension_worst      0\n",
       "Unnamed: 32                569\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast-Cancer Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.236405</td>\n",
       "      <td>1</td>\n",
       "      <td>1.097064</td>\n",
       "      <td>-2.073335</td>\n",
       "      <td>1.269934</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>1.568466</td>\n",
       "      <td>3.283515</td>\n",
       "      <td>2.652874</td>\n",
       "      <td>2.532475</td>\n",
       "      <td>...</td>\n",
       "      <td>1.886690</td>\n",
       "      <td>-1.359293</td>\n",
       "      <td>2.303601</td>\n",
       "      <td>2.001237</td>\n",
       "      <td>1.307686</td>\n",
       "      <td>2.616665</td>\n",
       "      <td>2.109526</td>\n",
       "      <td>2.296076</td>\n",
       "      <td>2.750622</td>\n",
       "      <td>1.937015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.236403</td>\n",
       "      <td>1</td>\n",
       "      <td>1.829821</td>\n",
       "      <td>-0.353632</td>\n",
       "      <td>1.685955</td>\n",
       "      <td>1.908708</td>\n",
       "      <td>-0.826962</td>\n",
       "      <td>-0.487072</td>\n",
       "      <td>-0.023846</td>\n",
       "      <td>0.548144</td>\n",
       "      <td>...</td>\n",
       "      <td>1.805927</td>\n",
       "      <td>-0.369203</td>\n",
       "      <td>1.535126</td>\n",
       "      <td>1.890489</td>\n",
       "      <td>-0.375612</td>\n",
       "      <td>-0.430444</td>\n",
       "      <td>-0.146749</td>\n",
       "      <td>1.087084</td>\n",
       "      <td>-0.243890</td>\n",
       "      <td>0.281190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.431741</td>\n",
       "      <td>1</td>\n",
       "      <td>1.579888</td>\n",
       "      <td>0.456187</td>\n",
       "      <td>1.566503</td>\n",
       "      <td>1.558884</td>\n",
       "      <td>0.942210</td>\n",
       "      <td>1.052926</td>\n",
       "      <td>1.363478</td>\n",
       "      <td>2.037231</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511870</td>\n",
       "      <td>-0.023974</td>\n",
       "      <td>1.347475</td>\n",
       "      <td>1.456285</td>\n",
       "      <td>0.527407</td>\n",
       "      <td>1.082932</td>\n",
       "      <td>0.854974</td>\n",
       "      <td>1.955000</td>\n",
       "      <td>1.152255</td>\n",
       "      <td>0.201391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.432121</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.768909</td>\n",
       "      <td>0.253732</td>\n",
       "      <td>-0.592687</td>\n",
       "      <td>-0.764464</td>\n",
       "      <td>3.283553</td>\n",
       "      <td>3.402909</td>\n",
       "      <td>1.915897</td>\n",
       "      <td>1.451707</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281464</td>\n",
       "      <td>0.133984</td>\n",
       "      <td>-0.249939</td>\n",
       "      <td>-0.550021</td>\n",
       "      <td>3.394275</td>\n",
       "      <td>3.893397</td>\n",
       "      <td>1.989588</td>\n",
       "      <td>2.175786</td>\n",
       "      <td>6.046041</td>\n",
       "      <td>4.935010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.432201</td>\n",
       "      <td>1</td>\n",
       "      <td>1.750297</td>\n",
       "      <td>-1.151816</td>\n",
       "      <td>1.776573</td>\n",
       "      <td>1.826229</td>\n",
       "      <td>0.280372</td>\n",
       "      <td>0.539340</td>\n",
       "      <td>1.371011</td>\n",
       "      <td>1.428493</td>\n",
       "      <td>...</td>\n",
       "      <td>1.298575</td>\n",
       "      <td>-1.466770</td>\n",
       "      <td>1.338539</td>\n",
       "      <td>1.220724</td>\n",
       "      <td>0.220556</td>\n",
       "      <td>-0.313395</td>\n",
       "      <td>0.613179</td>\n",
       "      <td>0.729259</td>\n",
       "      <td>-0.868353</td>\n",
       "      <td>-0.397100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0 -0.236405          1     1.097064     -2.073335        1.269934   0.984375   \n",
       "1 -0.236403          1     1.829821     -0.353632        1.685955   1.908708   \n",
       "2  0.431741          1     1.579888      0.456187        1.566503   1.558884   \n",
       "3  0.432121          1    -0.768909      0.253732       -0.592687  -0.764464   \n",
       "4  0.432201          1     1.750297     -1.151816        1.776573   1.826229   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0         1.568466          3.283515        2.652874             2.532475   \n",
       "1        -0.826962         -0.487072       -0.023846             0.548144   \n",
       "2         0.942210          1.052926        1.363478             2.037231   \n",
       "3         3.283553          3.402909        1.915897             1.451707   \n",
       "4         0.280372          0.539340        1.371011             1.428493   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...      1.886690      -1.359293         2.303601    2.001237   \n",
       "1  ...      1.805927      -0.369203         1.535126    1.890489   \n",
       "2  ...      1.511870      -0.023974         1.347475    1.456285   \n",
       "3  ...     -0.281464       0.133984        -0.249939   -0.550021   \n",
       "4  ...      1.298575      -1.466770         1.338539    1.220724   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0          1.307686           2.616665         2.109526              2.296076   \n",
       "1         -0.375612          -0.430444        -0.146749              1.087084   \n",
       "2          0.527407           1.082932         0.854974              1.955000   \n",
       "3          3.394275           3.893397         1.989588              2.175786   \n",
       "4          0.220556          -0.313395         0.613179              0.729259   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0        2.750622                 1.937015  \n",
       "1       -0.243890                 0.281190  \n",
       "2        1.152255                 0.201391  \n",
       "3        6.046041                 4.935010  \n",
       "4       -0.868353                -0.397100  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Copy the dataset and remove the last column\n",
    "breast_cancer_df = breast_cancer_df.iloc[:, :-1]\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_columns = breast_cancer_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_columns = breast_cancer_df.select_dtypes(include=['object', 'category']).columns\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the numeric columns\n",
    "breast_cancer_df[numeric_columns] = scaler.fit_transform(breast_cancer_df[numeric_columns])\n",
    "\n",
    "# Initialize LabelEncoder for categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_columns:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    breast_cancer_df[col] = label_encoders[col].fit_transform(breast_cancer_df[col].astype(str))\n",
    "\n",
    "# Display the processed DataFrame\n",
    "breast_cancer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "      <th>workclass_Federal-gov</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>226802</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>89814</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>336951</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>160323</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>103497</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  educational-num  gender  capital-gain  capital-loss  \\\n",
       "0   25  226802                7       1             0             0   \n",
       "1   38   89814                9       1             0             0   \n",
       "2   28  336951               12       1             0             0   \n",
       "3   44  160323               10       1          7688             0   \n",
       "4   18  103497               10       0             0             0   \n",
       "\n",
       "   hours-per-week  income  workclass_Federal-gov  workclass_Local-gov  ...  \\\n",
       "0              40       0                  False                False  ...   \n",
       "1              50       0                  False                False  ...   \n",
       "2              40       1                  False                 True  ...   \n",
       "3              40       1                  False                False  ...   \n",
       "4              30       0                  False                False  ...   \n",
       "\n",
       "   native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "0                    False                       False   \n",
       "1                    False                       False   \n",
       "2                    False                       False   \n",
       "3                    False                       False   \n",
       "4                    False                       False   \n",
       "\n",
       "   native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                    False                 False                  False   \n",
       "1                    False                 False                  False   \n",
       "2                    False                 False                  False   \n",
       "3                    False                 False                  False   \n",
       "4                    False                 False                  False   \n",
       "\n",
       "   native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                    False                           False   \n",
       "1                    False                           False   \n",
       "2                    False                           False   \n",
       "3                    False                           False   \n",
       "4                    False                           False   \n",
       "\n",
       "   native-country_United-States  native-country_Vietnam  \\\n",
       "0                          True                   False   \n",
       "1                          True                   False   \n",
       "2                          True                   False   \n",
       "3                          True                   False   \n",
       "4                          True                   False   \n",
       "\n",
       "   native-country_Yugoslavia  \n",
       "0                      False  \n",
       "1                      False  \n",
       "2                      False  \n",
       "3                      False  \n",
       "4                      False  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Identify column types\n",
    "binary_columns = [col for col in adult_df.select_dtypes(include=['object', 'category']).columns \n",
    "                  if adult_df[col].nunique() == 2]\n",
    "non_binary_columns = [col for col in adult_df.select_dtypes(include=['object', 'category']).columns \n",
    "                      if adult_df[col].nunique() > 2]\n",
    "numeric_columns = adult_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Initialize LabelEncoder for binary columns\n",
    "label_encoders = {}\n",
    "for col in binary_columns:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    adult_df[col] = label_encoders[col].fit_transform(adult_df[col].astype(str))\n",
    "\n",
    "# Apply One-Hot Encoding for non-binary columns\n",
    "adult_df = pd.get_dummies(adult_df, columns=non_binary_columns, drop_first=True)\n",
    "\n",
    "adult_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predefine a missing rate to be used across the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 0.1, 0.3, and 0.5.\n",
    "MISSING_RATE = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the helpers that will be used in model evaluation along Imputers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly it was implemented a function to introduce the Missingness Completely At Random (MCAR) missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdatagen.multivariate.mMCAR import mMCAR\n",
    "from mdatagen.multivariate.mMAR import mMAR\n",
    "from mdatagen.multivariate.mMNAR import mMNAR\n",
    "import pandas as pd\n",
    "\n",
    "def generate_multivariate_missingness(X, y, mechanism, missing_rate):\n",
    "    \"\"\"\n",
    "    General method to generate multivariate missingness using the mdatagen library.\n",
    "    \n",
    "    Parameters:\n",
    "    - X (pd.DataFrame): The feature matrix (independent variables).\n",
    "    - y (pd.Series or np.ndarray): The target variable.\n",
    "    - mechanism (str): The missingness mechanism ('MCAR', 'MAR', or 'MNAR').\n",
    "    - missing_rate (float): The proportion of values to replace with NaN (0.0 to 1.0).\n",
    "    - x_miss_list (list of str): The names of the columns to introduce missingness into.\n",
    "    - x_obs_list (list of str, optional): The names of the columns to condition on for MAR/MNAR mechanisms (if required).\n",
    "    - seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame of features with missing values introduced.\n",
    "    - pd.Series or np.ndarray: The unchanged target variable.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the appropriate generator based on the mechanism\n",
    "    if mechanism == 'MCAR':\n",
    "        generator = mMCAR(X=X, y=y,missing_rate=missing_rate)\n",
    "    elif mechanism == 'MAR':\n",
    "        generator = mMAR(X=X, y=y)\n",
    "    elif mechanism == 'MNAR':\n",
    "        generator = mMNAR(X=X, y=y)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mechanism '{mechanism}'. Choose from 'MCAR', 'MAR', or 'MNAR'.\")\n",
    "\n",
    "    # Generate the missing data\n",
    "    if mechanism == 'MCAR':\n",
    "        X_missing = generator.random()\n",
    "    elif mechanism == 'MAR':\n",
    "        X_missing = generator.correlated(missing_rate=missing_rate)\n",
    "    elif mechanism == 'MNAR':\n",
    "        X_missing = generator.correlated(missing_rate=missing_rate)\n",
    "    \n",
    "    # Display missingness details\n",
    "    global_missing_rate = X_missing.isnull().sum().sum() / X_missing.size\n",
    "    print(f\"Global Missing Rate = {global_missing_rate * 100:.2f}%\")\n",
    "    print(\"Missing values per column:\")\n",
    "    print(X_missing.isnull().sum())\n",
    "\n",
    "    return X_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, it is defined a helper `run_logistic_regression` function train, that evaluates a logistic regression model by imputing missing values in the training set and testing data using a specified imputer, fitting the model on the training data, predicting the labels for the testing data, and returning the predictions along with a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def run_logistic_regression(X_train_lr, X_test_lr, y_train_lr, y_test_lr, imputer):\n",
    "    X_train_lr = imputer.fit_transform(X_train_lr)\n",
    "    X_test_lr = imputer.fit_transform(X_test_lr)        # Maybe change to fit on training?\n",
    "\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train_lr, y_train_lr)\n",
    "    y_pred = clf.predict(X_test_lr)\n",
    "\n",
    "    report = classification_report(y_test_lr, y_pred)\n",
    "\n",
    "    print(report)\n",
    "    return y_pred, report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations calculation heatmap to Adult Dataset (ignore the following cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as stat\n",
    "\n",
    "# Function to check if a column is categorical\n",
    "def is_categorical(df, column):\n",
    "    \"\"\"Check if a column in a DataFrame is categorical.\"\"\"\n",
    "    return pd.api.types.is_object_dtype(df[column]) or pd.api.types.is_categorical_dtype(df[column])\n",
    "\n",
    "# Cramér's V function for categorical-categorical association, with missing value handling\n",
    "def cramers_v(x, y):\n",
    "    # Drop rows where either column has missing data\n",
    "    valid = pd.notnull(x) & pd.notnull(y)\n",
    "    x_clean, y_clean = x[valid], y[valid]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(x_clean, y_clean)\n",
    "    chi2 = stat.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    return np.sqrt(chi2 / (n * (min(confusion_matrix.shape) - 1)))\n",
    "\n",
    "# Spearman's rank correlation function, with missing value handling\n",
    "def spearmans_rank(x, y):\n",
    "    \"\"\"Calculate Spearman's rank correlation for continuous or ordinal variables, handling missing values.\"\"\"\n",
    "    # Drop rows where either column has missing data\n",
    "    valid = pd.notnull(x) & pd.notnull(y)\n",
    "    x_clean, y_clean = x[valid], y[valid]\n",
    "    \n",
    "    return stat.spearmanr(x_clean, y_clean)[0]  # Returns the correlation coefficient\n",
    "\n",
    "# Discretize numeric features\n",
    "def discretize_numeric(series, method=\"equal_width\", bins=4):\n",
    "    if method == \"equal_width\":\n",
    "        # Equal-width binning\n",
    "        return pd.cut(series, bins=bins, labels=False, include_lowest=True)\n",
    "    elif method == \"quantile\":\n",
    "        # Equal-frequency binning (quantiles)\n",
    "        return pd.qcut(series, q=bins, labels=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown discretization method: {method}\")\n",
    "\n",
    "# Cramér's V with discretized numeric features\n",
    "def cramers_v_discretized(x, y, method=\"equal_width\", bins=4):\n",
    "    # Determine which column is numeric and which is categorical\n",
    "    if pd.api.types.is_numeric_dtype(x):\n",
    "        x_discretized = discretize_numeric(x, method=method, bins=bins)\n",
    "        return cramers_v(x_discretized, y)\n",
    "    elif pd.api.types.is_numeric_dtype(y):\n",
    "        try:\n",
    "            y_discretized = discretize_numeric(y, method=method, bins=bins)\n",
    "            return cramers_v(x, y_discretized)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        raise ValueError(\"At least one of the inputs should be numeric for discretization.\")\n",
    "\n",
    "# Define the function to calculate the correlation based on data types\n",
    "def calculate_correlations(df, col1, col2):\n",
    "    \"\"\"Calculate correlation between two variables depending on their types.\"\"\"\n",
    "    if is_categorical(df, col1) and is_categorical(df, col2):\n",
    "        # Use Cramér's V for categorical-categorical variables\n",
    "        return cramers_v(df[col1], df[col2])\n",
    "    elif pd.api.types.is_numeric_dtype(df[col1]) and pd.api.types.is_numeric_dtype(df[col2]):\n",
    "        # Use Spearman's rank correlation for continuous-continuous or ordinal variables\n",
    "        return spearmans_rank(df[col1], df[col2])\n",
    "    elif (pd.api.types.is_numeric_dtype(df[col1]) and is_categorical(df, col2)) or \\\n",
    "         (is_categorical(df, col1) and pd.api.types.is_numeric_dtype(df[col2])):\n",
    "        # Use Cramér's V with discretized numeric variables\n",
    "        return cramers_v_discretized(df[col1], df[col2])\n",
    "    # Return NaN when there was an error on calculating the correlation\n",
    "    return np.nan\n",
    "\n",
    "# Correlation matrix calculation for the entire dataset\n",
    "def correlations(df):\n",
    "    df_columns = df.columns\n",
    "    correlation_matrix = pd.DataFrame(index=df_columns, columns=df_columns)\n",
    "\n",
    "    for i, col1 in enumerate(df_columns):\n",
    "        for col2 in df_columns[i+1:]:  # Use i+1 to avoid duplicate pairs and self-correlation\n",
    "            a_corr = calculate_correlations(df, col1, col2)\n",
    "            correlation_matrix.loc[col1, col2] = a_corr\n",
    "            correlation_matrix.loc[col2, col1] = a_corr \n",
    "\n",
    "    correlation_matrix = correlation_matrix.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Matrix for Adult Dataset')\n",
    "    plt.savefig(\"adult_correlations.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the target data for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_adult = adult_df.drop([\"income\"], axis=1)\n",
    "y_adult = adult_df[\"income\"]\n",
    "\n",
    "X_breast_cancer = breast_cancer_df.drop([\"diagnosis\"], axis=1)\n",
    "y_breast_cancer = breast_cancer_df[\"diagnosis\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missingness via Missign Completely At Random (MCAR) mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Missing Rate = 30.00%\n",
      "Missing values per column:\n",
      "id                         171\n",
      "radius_mean                162\n",
      "texture_mean               165\n",
      "perimeter_mean             169\n",
      "area_mean                  175\n",
      "smoothness_mean            168\n",
      "compactness_mean           171\n",
      "concavity_mean             187\n",
      "concave points_mean        180\n",
      "symmetry_mean              158\n",
      "fractal_dimension_mean     183\n",
      "radius_se                  168\n",
      "texture_se                 177\n",
      "perimeter_se               152\n",
      "area_se                    151\n",
      "smoothness_se              186\n",
      "compactness_se             177\n",
      "concavity_se               158\n",
      "concave points_se          182\n",
      "symmetry_se                150\n",
      "fractal_dimension_se       163\n",
      "radius_worst               176\n",
      "texture_worst              169\n",
      "perimeter_worst            169\n",
      "area_worst                 206\n",
      "smoothness_worst           163\n",
      "compactness_worst          186\n",
      "concavity_worst            170\n",
      "concave points_worst       178\n",
      "symmetry_worst             158\n",
      "fractal_dimension_worst    164\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_missing = generate_multivariate_missingness(X_breast_cancer, y_breast_cancer.values ,mechanism='MCAR', missing_rate=MISSING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(X_missing, y_breast_cancer\u001b[38;5;241m.\u001b[39mvalues, stratify\u001b[38;5;241m=\u001b[39my_breast_cancer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_missing, y_breast_cancer.values, stratify=y_breast_cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the dataset `X` and labels `y` into training and test sets (`X_train`, `X_test`, `y_train`, `y_test`) while preserving the original class distribution using stratified sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploring Mean Imputation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a `SimpleImputer` to fill missing values with the mean, and then train a decision model using the imputed data, and printing the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, SimpleImputer(missing_values=np.nan, strategy=\"mean\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploring kNN Imputation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than the mean, we can decide on a `KNNImputer` with `k=5` to fill missing values. Similarly, we train a logistic regression model using the imputed data, and print the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96        90\n",
      "           1       0.98      0.89      0.93        53\n",
      "\n",
      "    accuracy                           0.95       143\n",
      "   macro avg       0.96      0.94      0.95       143\n",
      "weighted avg       0.95      0.95      0.95       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, KNNImputer(n_neighbors=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94        90\n",
      "           1       0.94      0.83      0.88        53\n",
      "\n",
      "    accuracy                           0.92       143\n",
      "   macro avg       0.92      0.90      0.91       143\n",
      "weighted avg       0.92      0.92      0.91       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, IterativeImputer(max_iter=100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_missing = generate_multivariate_missingness(X_adult, y_adult.values ,mechanism='MCAR', missing_rate=MISSING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93        90\n",
      "           1       0.86      0.91      0.88        53\n",
      "\n",
      "    accuracy                           0.91       143\n",
      "   macro avg       0.90      0.91      0.90       143\n",
      "weighted avg       0.91      0.91      0.91       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, SimpleImputer(missing_values=np.nan, strategy=\"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95        90\n",
      "           1       0.91      0.92      0.92        53\n",
      "\n",
      "    accuracy                           0.94       143\n",
      "   macro avg       0.93      0.93      0.93       143\n",
      "weighted avg       0.94      0.94      0.94       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, KNNImputer(n_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92        90\n",
      "           1       0.84      0.91      0.87        53\n",
      "\n",
      "    accuracy                           0.90       143\n",
      "   macro avg       0.89      0.90      0.90       143\n",
      "weighted avg       0.90      0.90      0.90       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, IterativeImputer(max_iter=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Univariate Missingness via Missign At Random (MAR) mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_breast_cancer_ = X_breast_cancer.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncorrelation_matrix = X_breast_cancer.corr()\\n\\n# Plot heatmap\\nplt.figure(figsize=(12, 8))\\nsns.heatmap(correlation_matrix, annot=True, cmap=\\'coolwarm\\')\\nplt.title(\"Feature Correlation Heatmap for Breast Cancer Dataset\")\\n\\n# Save the heatmap as an image file\\nplt.savefig(\"breast_cancer_correlation_heatmap.png\", dpi=300, bbox_inches=\\'tight\\')\\n\\n# Display the heatmap\\nplt.show()\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "correlation_matrix = X_breast_cancer.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Feature Correlation Heatmap for Breast Cancer Dataset\")\n",
    "\n",
    "# Save the heatmap as an image file\n",
    "plt.savefig(\"breast_cancer_correlation_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Missing Rate = 30.00%\n",
      "Missing values per column:\n",
      "id                         174\n",
      "radius_mean                187\n",
      "texture_mean               175\n",
      "perimeter_mean             165\n",
      "area_mean                  168\n",
      "smoothness_mean            156\n",
      "compactness_mean           164\n",
      "concavity_mean             167\n",
      "concave points_mean        178\n",
      "symmetry_mean              166\n",
      "fractal_dimension_mean     176\n",
      "radius_se                  175\n",
      "texture_se                 178\n",
      "perimeter_se               165\n",
      "area_se                    157\n",
      "smoothness_se              162\n",
      "compactness_se             182\n",
      "concavity_se               164\n",
      "concave points_se          183\n",
      "symmetry_se                182\n",
      "fractal_dimension_se       172\n",
      "radius_worst               177\n",
      "texture_worst              160\n",
      "perimeter_worst            154\n",
      "area_worst                 180\n",
      "smoothness_worst           179\n",
      "compactness_worst          174\n",
      "concavity_worst            170\n",
      "concave points_worst       167\n",
      "symmetry_worst             167\n",
      "fractal_dimension_worst    168\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_missing = generate_multivariate_missingness(X=X_breast_cancer_,y=y_breast_cancer.values,mechanism='MAR',missing_rate=MISSING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_missing, y_breast_cancer, stratify=y_breast_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.81      0.87        90\n",
      "           1       0.74      0.91      0.81        53\n",
      "\n",
      "    accuracy                           0.85       143\n",
      "   macro avg       0.84      0.86      0.84       143\n",
      "weighted avg       0.86      0.85      0.85       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, SimpleImputer(missing_values=np.nan, strategy=\"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.89      0.91        90\n",
      "           1       0.82      0.89      0.85        53\n",
      "\n",
      "    accuracy                           0.89       143\n",
      "   macro avg       0.88      0.89      0.88       143\n",
      "weighted avg       0.89      0.89      0.89       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, KNNImputer(n_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        90\n",
      "           1       0.91      0.91      0.91        53\n",
      "\n",
      "    accuracy                           0.93       143\n",
      "   macro avg       0.93      0.93      0.93       143\n",
      "weighted avg       0.93      0.93      0.93       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, IterativeImputer(max_iter=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adult_ = X_adult.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlations(adult_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Missing Rate = 30.00%\n",
      "Missing values per column:\n",
      "age                               14728\n",
      "fnlwgt                            14833\n",
      "educational-num                   14667\n",
      "gender                            14442\n",
      "capital-gain                      14731\n",
      "                                  ...  \n",
      "native-country_Thailand           14710\n",
      "native-country_Trinadad&Tobago    14679\n",
      "native-country_United-States      14740\n",
      "native-country_Vietnam            14590\n",
      "native-country_Yugoslavia         14624\n",
      "Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_missing = generate_multivariate_missingness(X=X_adult_, y=y_adult.values, mechanism='MAR',missing_rate=MISSING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_missing, y_adult, stratify=y_adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85      9289\n",
      "           1       0.52      0.54      0.53      2922\n",
      "\n",
      "    accuracy                           0.77     12211\n",
      "   macro avg       0.69      0.69      0.69     12211\n",
      "weighted avg       0.78      0.77      0.77     12211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, SimpleImputer(missing_values=np.nan, strategy=\"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86      9289\n",
      "           1       0.54      0.55      0.54      2922\n",
      "\n",
      "    accuracy                           0.78     12211\n",
      "   macro avg       0.70      0.70      0.70     12211\n",
      "weighted avg       0.78      0.78      0.78     12211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, KNNImputer(n_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.84      9289\n",
      "           1       0.51      0.56      0.54      2922\n",
      "\n",
      "    accuracy                           0.77     12211\n",
      "   macro avg       0.68      0.70      0.69     12211\n",
      "weighted avg       0.77      0.77      0.77     12211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, IterativeImputer(max_iter=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Univariate Missingness via Missign Not At Random (MNAR) mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Missing Rate = 29.97%\n",
      "Missing values per column:\n",
      "id                           0\n",
      "radius_mean                256\n",
      "texture_mean                 0\n",
      "perimeter_mean             256\n",
      "area_mean                    0\n",
      "smoothness_mean              0\n",
      "compactness_mean             0\n",
      "concavity_mean               0\n",
      "concave points_mean        341\n",
      "symmetry_mean                0\n",
      "fractal_dimension_mean       0\n",
      "radius_se                  341\n",
      "texture_se                   0\n",
      "perimeter_se                 0\n",
      "area_se                    341\n",
      "smoothness_se              341\n",
      "compactness_se             341\n",
      "concavity_se                 0\n",
      "concave points_se          341\n",
      "symmetry_se                  0\n",
      "fractal_dimension_se         0\n",
      "radius_worst                 0\n",
      "texture_worst              341\n",
      "perimeter_worst            341\n",
      "area_worst                 341\n",
      "smoothness_worst           341\n",
      "compactness_worst            0\n",
      "concavity_worst            341\n",
      "concave points_worst       341\n",
      "symmetry_worst             341\n",
      "fractal_dimension_worst    341\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = X_breast_cancer.copy()\n",
    "y = y_breast_cancer\n",
    "\n",
    "X_missing = generate_multivariate_missingness(X=X, y=y.values, mechanism='MNAR',missing_rate=MISSING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_missing, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92        90\n",
      "           1       0.84      0.91      0.87        53\n",
      "\n",
      "    accuracy                           0.90       143\n",
      "   macro avg       0.89      0.90      0.90       143\n",
      "weighted avg       0.90      0.90      0.90       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, SimpleImputer(missing_values=np.nan, strategy=\"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90        90\n",
      "           1       0.82      0.87      0.84        53\n",
      "\n",
      "    accuracy                           0.88       143\n",
      "   macro avg       0.87      0.88      0.87       143\n",
      "weighted avg       0.88      0.88      0.88       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, KNNImputer(n_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95        90\n",
      "           1       0.92      0.91      0.91        53\n",
      "\n",
      "    accuracy                           0.94       143\n",
      "   macro avg       0.93      0.93      0.93       143\n",
      "weighted avg       0.94      0.94      0.94       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, IterativeImputer(max_iter=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_adult.copy()\n",
    "y = y_adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n",
      "/home/andre/.local/lib/python3.10/site-packages/mdatagen/multivariate/mMNAR.py:246: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  dataset_chunk.loc[pos_xmiss, val] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Missing Rate = 30.00%\n",
      "Missing values per column:\n",
      "age                               29305\n",
      "fnlwgt                                0\n",
      "educational-num                   29305\n",
      "gender                            29305\n",
      "capital-gain                      29305\n",
      "                                  ...  \n",
      "native-country_Thailand               0\n",
      "native-country_Trinadad&Tobago        0\n",
      "native-country_United-States          0\n",
      "native-country_Vietnam                0\n",
      "native-country_Yugoslavia             0\n",
      "Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_missing = generate_multivariate_missingness(X=X, y=y.values, mechanism='MNAR',missing_rate=MISSING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_missing, y, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86      9289\n",
      "           1       0.55      0.61      0.58      2922\n",
      "\n",
      "    accuracy                           0.79     12211\n",
      "   macro avg       0.71      0.73      0.72     12211\n",
      "weighted avg       0.80      0.79      0.79     12211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, SimpleImputer(missing_values=np.nan, strategy=\"mean\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86      9289\n",
      "           1       0.57      0.58      0.58      2922\n",
      "\n",
      "    accuracy                           0.79     12211\n",
      "   macro avg       0.72      0.72      0.72     12211\n",
      "weighted avg       0.80      0.79      0.80     12211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, KNNImputer(n_neighbors=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred, clf_report \u001b[38;5;241m=\u001b[39m \u001b[43mrun_logistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterativeImputer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mrun_logistic_regression\u001b[0;34m(X_train_lr, X_test_lr, y_train_lr, y_test_lr, imputer)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_logistic_regression\u001b[39m(X_train_lr, X_test_lr, y_train_lr, y_test_lr, imputer):\n\u001b[0;32m---> 11\u001b[0m     X_train_lr \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_lr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     X_test_lr \u001b[38;5;241m=\u001b[39m imputer\u001b[38;5;241m.\u001b[39mfit_transform(X_test_lr)        \u001b[38;5;66;03m# Maybe change to fit on training?\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     clf \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/impute/_iterative.py:789\u001b[0m, in \u001b[0;36mIterativeImputer.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat_idx \u001b[38;5;129;01min\u001b[39;00m ordered_idx:\n\u001b[1;32m    786\u001b[0m     neighbor_feat_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_neighbor_feat_idx(\n\u001b[1;32m    787\u001b[0m         n_features, feat_idx, abs_corr_mat\n\u001b[1;32m    788\u001b[0m     )\n\u001b[0;32m--> 789\u001b[0m     Xt, estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impute_one_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_missing_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeat_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneighbor_feat_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m     estimator_triplet \u001b[38;5;241m=\u001b[39m _ImputerTriplet(\n\u001b[1;32m    799\u001b[0m         feat_idx, neighbor_feat_idx, estimator\n\u001b[1;32m    800\u001b[0m     )\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimputation_sequence_\u001b[38;5;241m.\u001b[39mappend(estimator_triplet)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/impute/_iterative.py:418\u001b[0m, in \u001b[0;36mIterativeImputer._impute_one_feature\u001b[0;34m(self, X_filled, mask_missing_values, feat_idx, neighbor_feat_idx, estimator, fit_mode, params)\u001b[0m\n\u001b[1;32m    408\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m _safe_indexing(\n\u001b[1;32m    409\u001b[0m         _safe_indexing(X_filled, neighbor_feat_idx, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;241m~\u001b[39mmissing_row_mask,\n\u001b[1;32m    411\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    412\u001b[0m     )\n\u001b[1;32m    413\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m _safe_indexing(\n\u001b[1;32m    414\u001b[0m         _safe_indexing(X_filled, feat_idx, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;241m~\u001b[39mmissing_row_mask,\n\u001b[1;32m    416\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    417\u001b[0m     )\n\u001b[0;32m--> 418\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# if no missing values, don't predict\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(missing_row_mask) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/linear_model/_bayes.py:294\u001b[0m, in \u001b[0;36mBayesianRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# Convergence loop of the bayesian ridge regression\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# update posterior mean coef_ based on alpha_ and lambda_ and\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# compute corresponding rmse\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m     coef_, rmse_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_coef_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXT_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_vals_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_score:\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;66;03m# compute the log marginal likelihood\u001b[39;00m\n\u001b[1;32m    299\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_marginal_likelihood(\n\u001b[1;32m    300\u001b[0m             n_samples, n_features, eigen_vals_, alpha_, lambda_, coef_, rmse_\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/linear_model/_bayes.py:384\u001b[0m, in \u001b[0;36mBayesianRidge._update_coef_\u001b[0;34m(self, X, y, n_samples, n_features, XT_y, U, Vh, eigen_vals_, alpha_, lambda_)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update posterior mean and compute corresponding rmse.\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03mPosterior mean is given by coef_ = scaled_sigma_ * X.T * y where\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03mscaled_sigma_ = (lambda_/alpha_ * np.eye(n_features)\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m                 + np.dot(X.T, X))^-1\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m>\u001b[39m n_features:\n\u001b[0;32m--> 384\u001b[0m     coef_ \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_dot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mVh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43meigen_vals_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXT_y\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     coef_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mmulti_dot(\n\u001b[1;32m    389\u001b[0m         [X\u001b[38;5;241m.\u001b[39mT, U \u001b[38;5;241m/\u001b[39m (eigen_vals_ \u001b[38;5;241m+\u001b[39m lambda_ \u001b[38;5;241m/\u001b[39m alpha_)[\u001b[38;5;28;01mNone\u001b[39;00m, :], U\u001b[38;5;241m.\u001b[39mT, y]\n\u001b[1;32m    390\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/linalg/linalg.py:2750\u001b[0m, in \u001b[0;36mmulti_dot\u001b[0;34m(arrays, out)\u001b[0m\n\u001b[1;32m   2748\u001b[0m \u001b[38;5;66;03m# _multi_dot_three is much faster than _multi_dot_matrix_chain_order\u001b[39;00m\n\u001b[1;32m   2749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m-> 2750\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_multi_dot_three\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2751\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2752\u001b[0m     order \u001b[38;5;241m=\u001b[39m _multi_dot_matrix_chain_order(arrays)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/linalg/linalg.py:2782\u001b[0m, in \u001b[0;36m_multi_dot_three\u001b[0;34m(A, B, C, out)\u001b[0m\n\u001b[1;32m   2780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dot(dot(A, B), C, out\u001b[38;5;241m=\u001b[39mout)\n\u001b[1;32m   2781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_pred, clf_report = run_logistic_regression(X_train, X_test, y_train, y_test, IterativeImputer(max_iter=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
